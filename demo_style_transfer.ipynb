{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-863722a76030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneural_style\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_style_transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PyTorch-deep-photo-styletransfer/neural_style.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclosed_form_matting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_laplacian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimage_preprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_to_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_to_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PyTorch-deep-photo-styletransfer/closed_form_matting.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride_tricks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_strided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_style import run_style_transfer\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imsize = (512, 512) if torch.cuda.is_available() else (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensor_to_image(x):\n",
    "    \"\"\"\n",
    "    Transforms torch.Tensor to np.array\n",
    "        (1, C, W, H) -> (W, H, C)\n",
    "        (B, C, W, H) -> (B, W, H, C) \n",
    "\n",
    "    \"\"\"\n",
    "    return x.detach().numpy().transpose(0, 2, 3, 1).squeeze().clip(0, 1)\n",
    "\n",
    "\n",
    "def image_to_tensor(x):\n",
    "    \"\"\"\n",
    "    Transforms np.array to torch.Tensor\n",
    "        (W, H)       -> (1, 1, W, H)\n",
    "        (W, H, C)    -> (1, C, W, H)\n",
    "        (B, W, H, C) -> (B, C, W, H)\n",
    "\n",
    "    \"\"\"\n",
    "    if x.ndim == 2:\n",
    "        return torch.Tensor(x).unsqueeze(0).unsqueeze(0)\n",
    "    if x.ndim == 3:\n",
    "        return torch.Tensor(x.transpose(2, 0, 1)).unsqueeze(0)\n",
    "    if x.ndim == 4:\n",
    "        return torch.Tensor(x.transpose(0, 3, 1, 2))\n",
    "    raise RuntimeError(\"np.array's ndim is out of range 2, 3 or 4.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_images(\n",
    "    style_img,\n",
    "    output_img,\n",
    "    content_img,\n",
    "    style_title=\"Style Image\",\n",
    "    output_title=\"Output Image\",\n",
    "    content_title=\"Content Image\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots style, output and content images to ease comparison.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(tensor_to_image(style_img))\n",
    "    plt.title(\"Style Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(tensor_to_image(output_img))\n",
    "    plt.title(\"Output Image\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(tensor_to_image(content_img))\n",
    "    plt.title(\"Content Image\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "imsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize((1024,1024)),  # scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "style_img = image_loader(\"./examples/style/tar1.png\")\n",
    "content_img = image_loader(\"./examples/input/in1.png\")\n",
    "input_img = content_img.clone()\n",
    "assert style_img.size() == content_img.size(), \\\n",
    "    \"we need to import style and content images of the same size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_images(style_img, input_img, content_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "vgg_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "vgg_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "style_layers = [\"conv1_1\", \"conv2_1\", \"conv3_1\", \"conv4_1\", \"conv5_1\"]\n",
    "content_layers = [\"conv4_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_style_transfer(\n",
    "    vgg,\n",
    "    vgg_normalization_mean,\n",
    "    vgg_normalization_std,\n",
    "    style_layers,\n",
    "    content_layers,\n",
    "    style_img,\n",
    "    content_img,\n",
    "    input_img,\n",
    "    style_masks,\n",
    "    content_masks,\n",
    "    device,\n",
    "    reg=False,\n",
    "    style_weight=1e6,\n",
    "    content_weight=1e4,\n",
    "    reg_weight=0,\n",
    "    num_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
